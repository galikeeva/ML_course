{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "# Machine Learning\n",
    "# Algorithms for Recommenders\n",
    "# Implementation of libFM in Keras\n",
    "\n",
    "\n",
    "## Yuri Balasanov, Leonid Nazarov, &copy; iLykei 2019\n",
    "\n",
    "\n",
    "This notebook shows how to implement Libfm in Keras. The approach has been suggested by [Jean Francois Puget](https://www.ibm.com/developerworks/community/profiles/html/profileView.do?key=9bff030e-4d8e-47a0-a566-8689de958f72#&tabinst=Updates). We use partialy his [blog post](https://www.ibm.com/developerworks/community/blogs/jfp/entry/Implementing_Libfm_in_Keras?lang=en) below.  \n",
    "Underlying theory is presented on the slides of Lecture 9 **Recommenders**. We recommend also read the seminal paper on Factorization Machines:\n",
    "\n",
    "Steffen Rendle (2010): <i><a href=\"https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\">Factorization Machines</a></i>, in Proceedings of the 10th IEEE International Conference on Data Mining (ICDM 2010), Sydney, Australia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import gc\n",
    "\n",
    "import mlcrate as mlc\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "from keras import regularizers \n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a direct implementation of Libfm, with a first embedding layer, then a smart computation of all pairwise dot products.  \n",
    "The smartness comes direclty from the paper by [Rendle](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf).  \n",
    "\n",
    "Jean Francois Puget generalized the model to allow for non-categorical input, in which case he uses a dense layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Factorization Machines method to the **movielens-100k** dataset. MovieLens is a benchmark dataset for recommender systems. \n",
    "\n",
    "The task is to predict ratings for movies. \n",
    "\n",
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating\n",
       "0   196    242       3\n",
       "1   186    302       3\n",
       "2    22    377       1\n",
       "3   244     51       2\n",
       "4   166    346       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_movielens = pd.read_csv('u.data', sep='\\t',\n",
    "                   names=['user', 'movie', 'rating', 'timestamp'], header=None)\n",
    "data_movielens.drop('timestamp',axis=1,inplace=True)\n",
    "print(data_movielens.shape)\n",
    "data_movielens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before turning to the Movielens-100k look at the following toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating\n",
       "0     1      1       5\n",
       "1     1      2       3\n",
       "2     1      3       1\n",
       "3     2      3       3\n",
       "4     2      4       5\n",
       "5     3      1       1\n",
       "6     3      3       3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('small_example.data', sep='\\t',\n",
    "                   names=['user', 'movie', 'rating', 'timestamp'], header=None)\n",
    "data.drop('timestamp',axis=1,inplace=True)\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating is interpreted as interaction between 'user' and 'movie'.\n",
    "\n",
    "Calculate upper bound of each feature to prepare for the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['user', 'movie']\n",
    "f_size  = [int(data[f].max()) + 1 for f in features]\n",
    "f_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data contain not all combinations of the first two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12 > 7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for f in features:\n",
    "    print(len(data[f].unique()))\n",
    "f'{len(data.user.unique())*len(data.movie.unique())} > {len(data)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test part. The target column is `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([3, 1, 3, 2]), array([1, 1, 3, 3])]\n",
      "[array([1, 1, 2]), array([2, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "train,test,y_train,y_test = train_test_split(data, data.rating.values, \n",
    "                                    stratify=data.rating.values, test_size=0.3)\n",
    "X_train = [train[f].values for f in features]\n",
    "X_test = [test[f].values for f in features]\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "The first step in the analysis is **embedding**, i.e. encoding each input with a vector.\n",
    "\n",
    "Create a function that takes one level of multilevel categorical variable and encodes it with a level of `k_latent` numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_latent = 2\n",
    "embedding_reg = 0.0002 \n",
    "kernel_reg = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix of weights to illustrate the work of the embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]]\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = np.arange(12).reshape((-1,2))\n",
    "print(weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function embedding layer transforms 1-dimensional categorical varialble `x_input` (for example, user ID or item ID) into `output_dim`-dimensional vector. Then the result gets flattened and returned as tensor.\n",
    "\n",
    "Arguments of the layer are:\n",
    "\n",
    "- `input_dim`: size of the vocabulary\n",
    "- `output_dim`: dimension of the vector space in which words are embedded\n",
    "- `input_length`: length of input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim=2\n",
    "def get_embed(x_input, x_size, out_dim,test_weights=False):\n",
    "    # x_input is index of input (either user or item)\n",
    "    # x_size is length of vocabulary (e.g. total number of users or items)\n",
    "    # test_weights is a demo flag to show results with predefined weights\n",
    "    # out_dim is size of embedding vectors\n",
    "    if x_size > 0: #category\n",
    "        if test_weights & (out_dim<=2):\n",
    "            embed = Embedding(x_size, out_dim, input_length=1,\n",
    "                          weights=[weight_matrix[:x_size,:out_dim]], \n",
    "                          embeddings_regularizer=l2(embedding_reg))(x_input)\n",
    "        else:\n",
    "            embed = Embedding(x_size, out_dim, input_length=1,\n",
    "                              embeddings_regularizer=l2(embedding_reg))(x_input)\n",
    "        embed = Flatten()(embed)\n",
    "    else:\n",
    "        embed = Dense(out_dim, kernel_regularizer=l2(embedding_reg))(x_input)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate working of the function create a list of inputs `X` and make a model with the output tensor generated by `get_embed()` with predefined weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1, 2)              8         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = [2,3,2]\n",
    "inp = Input(shape=(1,))\n",
    "emd_model = Model(inp,get_embed(inp,max(X)+1,out_dim,test_weights=True))\n",
    "plot_model(emd_model, to_file='emb_model.png',\n",
    "           show_shapes=True,show_layer_names=True)\n",
    "emd_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Embedding Model](emb_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  5.],\n",
       "       [ 6.,  7.],\n",
       "       [ 4.,  5.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is the list of rows of `weight_matrix` corresponding to values of `X`.\n",
    "\n",
    "Note that `X[0]` and `X[2]` are equal and generated identical vectors.\n",
    "\n",
    "To achieve the same result for non-categorical input set `x_size=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63004494,  0.36816451],\n",
       "       [ 0.77164435,  0.45090762],\n",
       "       [ 0.63004494,  0.36816451]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model(inp,get_embed(inp,0,out_dim)).predict(np.sqrt(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LibFM model implementation in Keras\n",
    "\n",
    "### Some useful Keras layers and objects\n",
    "\n",
    "This section explains some layers and objects used in the LibFM implementation in the following section.\n",
    "\n",
    "#### Zip object\n",
    "\n",
    "Implementation code of LibFM below uses `zip` object which makes an iterator that aggregates elements from each of the iterables and returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. Elements are loaded into memory on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range as list:  [0, 1, 2]\n",
      "String as list:  ['a', 'b', 'c', 'd']\n",
      "Zip object as list:  [(0, 'a'), (1, 'b'), (2, 'c')]\n"
     ]
    }
   ],
   "source": [
    "print('Range as list: ',list(range(3)))\n",
    "print('String as list: ',list('abcd'))\n",
    "print('Zip object as list: ',list(zip(range(3), 'abcd')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Add\n",
    "\n",
    "Layer `Add` takes as input a list of tensors,\n",
    "all of the same shape, adds them and returns\n",
    "a single tensor (also of the same shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = keras.layers.Input(shape=(16,))\n",
    "x1 = keras.layers.Dense(8, activation='relu')(input1)\n",
    "input2 = keras.layers.Input(shape=(32,))\n",
    "x2 = keras.layers.Dense(8, activation='relu')(input2)\n",
    "added = keras.layers.Add()([x1, x2])\n",
    "\n",
    "out = keras.layers.Dense(4)(added)\n",
    "model = keras.models.Model(inputs=[input1, input2], outputs=out)\n",
    "plot_model(model, to_file='modelAdd.png',show_shapes=True,\n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Add](modelAdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Subtract\n",
    "\n",
    "Layer `Subtract` takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = keras.layers.Input(shape=(16,))\n",
    "x1 = keras.layers.Dense(8, activation='relu')(input1)\n",
    "input2 = keras.layers.Input(shape=(32,))\n",
    "x2 = keras.layers.Dense(8, activation='relu')(input2)\n",
    "# Equivalent to subtracted = keras.layers.subtract([x1, x2])\n",
    "subtracted = keras.layers.Subtract()([x1, x2])\n",
    "\n",
    "out = keras.layers.Dense(4)(subtracted)\n",
    "model = keras.models.Model(inputs=[input1, input2], outputs=out)\n",
    "plot_model(model, to_file='modelSubtract.png',show_shapes=True,\n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Subtract](modelSubtract.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Dot\n",
    "\n",
    "Layer `Dot` computes a dot product between samples in two tensors.\n",
    "\n",
    "E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1).\n",
    "\n",
    "Arguments\n",
    "\n",
    "- axes: Integer or tuple of integers, axis or axes along which to take the dot product.\n",
    "- normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of LibFM\n",
    "\n",
    "#### Model\n",
    "\n",
    "Follow all steps of LibFM on a simple example.\n",
    "\n",
    "The general LibFM model is a linear model with interactions:\n",
    "\n",
    "$$\\hat{y}_{j,h} = w_0+\\sum_{j=1}^{n_u} w_j^{(u)}x_j^{(u)} +\\sum_{h=1}^{n_i}w_h^{(i)}x_h^{(i)} + \\sum_{j}\\sum_{h} w_{j,h} x_j^{(u)}x_h^{(i)},$$\n",
    "where: \n",
    "\n",
    "- $\\hat{y}_{j,h}$ is predicted response, i.e. rating of item $h$ by user $j$\n",
    "- $x_j^{(u)}$ is the dummy variable for user $j$;\n",
    "- $x_h^{(i)}$ is the dummy variable for item $h$;\n",
    "- $w_j^{(u)}$ is the main effect of user $j$;\n",
    "- $w_h^{(i)}$ is the main effect of item $h$;\n",
    "- $w_{j,h}$ is the interaction between user $j$ and item $h$.\n",
    "\n",
    "Factorization with $k$ factors in the formula above applies to the matrix of interactions $\\{ w_{j,h} \\}$:\n",
    "$$w_{j,h}=\\sum_{g=1}^k v_{j,g}^{(u)}v_{h,g}^{(i)},$$\n",
    "where \n",
    "$$v_j^{(u)}=(v_{j,1}^{(u)},\\ldots,v_{j,k}^{(u))})$$ \n",
    "is the vector of factors for user $j$ and \n",
    "$$v_h^{(i)}=(v_{h,1}^{(i)},\\ldots,v_{h,k}^{(i)})$$ \n",
    "is the vector of factors for item $h$.\n",
    "\n",
    "In case of one active user $j_{u_a}$ and one active item $h_{j_a}$ the model simplifies to\n",
    "$$\\hat{y}_{j_{u_a},h_{i_a}} = w_0+w_{j_{u_a}}^{(u)}+w_{h_{i_a}}^{(i)} +  w_{j_{u_a},h_{i_a}},$$\n",
    "where\n",
    "$$w_{j_{u_a},h_{i_a}}=\\sum_{g=1}^k v_{j_{u_a},g}^{(u)}v_{h_{i_a},g}^{(i)}.$$\n",
    "\n",
    "#### Computation of interactions\n",
    "\n",
    "It is possible to organize computation of interaction terms in a very efficient way and reduce complexity of computation from $O(kn^2)$ to only $O(kn)$.\n",
    "\n",
    "The idea is the following: instead of calculation  \n",
    "$$F = \\sum _{j<h} (f_j \\cdot f_h),$$ \n",
    "which has complexity $O(kn^2)$, calculate \n",
    "$$F =\\frac{1}{2} \\sum_{j} (f_j \\cdot (S - f_j)),$$\n",
    "where $(a \\cdot b)$ is dot product of vectors $a$ and $b$ and $S=\\sum_j f_j$.\n",
    "Alternative calculation requires only $n$ dot products and has complexity $O(kn)$. \n",
    "\n",
    "Lemma in the [paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) applies the alternative calculation for $f_j=v_{j}^{(u)}x_j^{(u)},f_h=v_{h}^{(i)}x_h^{(i)}$\n",
    "\n",
    "In particular:\n",
    "$$\\sum_{j}\\sum_{h} \\left(v_{j}^{(u)} \\cdot v_{h}^{(i)}\\right) x_j^{(u)}x_h^{(i)}=\\sum_{j}\\sum_{h}\\left( \\sum_{g=1}^k \\left(v_{j,g}^{(u)}x_j^{(u)}\\right) \\left(v_{h,g}^{(i)}x_h^{(i)} \\right) \\right)$$\n",
    "$$=\\sum_{j}\\sum_{h}\\left( \\left(v_{j,g}^{(u)}x_j^{(u)}\\right) \\cdot \\left(v_{h,g}^{(i)}x_h^{(i)} \\right) \\right) $$\n",
    "So, calculation of interactions is done with dot products of factor vectors $f_j$ and vectors $S=\\sum_g f_g-f_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Input layer\n",
    "\n",
    "Create input as list of 2 tensors representing index of user $u_a$ and index of item $i_a$. Each tensor has shape `(None,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_6:0' shape=(?, 1) dtype=float32>,\n",
       " <tf.Tensor 'input_7:0' shape=(?, 1) dtype=float32>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_input = len(f_size)\n",
    "    \n",
    "input_x = [Input(shape=(1,)) for i in range(dim_input)]\n",
    "input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Main effects\n",
    "\n",
    "Create a list of tensors for linear terms $\\sum_{j=1}^{n_u} w_j^{(u)}x_j^{(u)}, \\sum_{h=1}^{n_i}w_h^{(i)}x_h^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'flatten_2/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_3/Reshape:0' shape=(?, ?) dtype=float32>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_terms = [get_embed(x, size, 1) for (x, size) in zip(input_x, f_size)]\n",
    "lin_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is list of terms \n",
    "$$ \\left[ w_1^{(u)}x_1^{(u)},\\ldots,w_{n_u}^{(u)}x_{n_u}^{(u)},w_1^{(i)}x_1^{(i)},\\ldots,w_{n_i}^{(i)}x_{n_i}^{(i)} \\right] .$$\n",
    "\n",
    "In case of collaborative filtering with dummy variables for inputs the list reduces to $\\left[ w_{j_{u_a}}^{(u)},w_{h_{i_a}}^{(i)} \\right].$\n",
    "\n",
    "Number of trained coefficients in this layer is $n_u+n_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(f_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 1)         4           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 1)         5           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1)            0           embedding_5[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_x = [Input(shape=(1,)) for i in range(dim_input)] \n",
    "    \n",
    "lin_terms = [get_embed(x, size, 1) for (x, size) in zip(input_x, f_size)]\n",
    "model = Model(inputs=input_x, outputs=lin_terms)\n",
    "plot_model(model, to_file='modelLinterms.png',show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Linterms](modelLinterms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check predicted linear terms with randomly initiated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_terms=model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the user input of `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03469365],\n",
       "       [-0.02713688],\n",
       "       [ 0.03469365],\n",
       "       [ 0.01670531]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_terms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the user input of `X_train`, user weights of the model and reconstruct predictions of linear terms for user using this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User index:\n",
      " [3 1 3 2]\n",
      "User weights:\n",
      " [ 0.01672569 -0.02713688  0.01670531  0.03469365]\n",
      "Reconstructed user linear terms:\n",
      "0.0346936\n",
      "-0.0271369\n",
      "0.0346936\n",
      "0.0167053\n"
     ]
    }
   ],
   "source": [
    "print('User index:\\n',X_train[0])\n",
    "print('User weights:\\n',np.concatenate(model.layers[2].get_weights()[0])) #list of lists to list\n",
    "print('Reconstructed user linear terms:')\n",
    "for User in X_train[0]: print(np.concatenate(model.layers[2].get_weights()[0])[User])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above shows that prediction is the weight of the model indexed by the value of the input.\n",
    "\n",
    "Same way reconstruct predictions of the linear terms for item. Note that item weights are in layer 3, the second embedding layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0396462 ]\n",
      " [ 0.0396462 ]\n",
      " [-0.00811861]\n",
      " [-0.00811861]]\n",
      "[1 1 3 3]\n",
      "[-0.0155157   0.0396462  -0.0332055  -0.00811861 -0.04376755]\n",
      "Reconstructed item linear terms:\n",
      "0.0396462\n",
      "0.0396462\n",
      "-0.00811861\n",
      "-0.00811861\n"
     ]
    }
   ],
   "source": [
    "print(lin_terms[1])\n",
    "print(X_train[1])\n",
    "print(np.concatenate(model.layers[3].get_weights()[0])) #list of lists to list\n",
    "print('Reconstructed item linear terms:')\n",
    "for User in X_train[1]: print(np.concatenate(model.layers[3].get_weights()[0])[User])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Factors\n",
    "\n",
    "Creating factors $v_j x_j$ for users and items is similar to creating main effects, but dimension of embedding space is equal to the selected number of factors `k_latent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'flatten_6/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_7/Reshape:0' shape=(?, ?) dtype=float32>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors = [get_embed(x, size, k_latent) for (x, size) in zip(input_x, f_size)]\n",
    "factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of this layer is list of 2 vectors:\n",
    "$$ \\left[ v_j^{(u)},v_h^{(i)} \\right]= \\left[ (v_{j,1}^{(u)},\\ldots,v_{j,k}^{(u)}),(v_{h,1}^{(i)},\\ldots,v_{h,k}^{(i)}) \\right],$$\n",
    "where $k$ is the selected number of factors `k_latent`.\n",
    "\n",
    "In the example below $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 2)         8           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 2)         10          input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2)            0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 2)            0           embedding_9[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 18\n",
      "Trainable params: 18\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_x = [Input(shape=(1,)) for i in range(dim_input)] \n",
    "    \n",
    "factors = [get_embed(x, size, k_latent) for (x, size) in zip(input_x, f_size)]\n",
    "model = Model(inputs=input_x, outputs=factors)\n",
    "plot_model(model, to_file='modelFactors.png',show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Factors](modelFactors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict factors using the model and reconstruct them with randomly initiated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "factrs=model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For user factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User index:\n",
      " [3 1 3 2]\n",
      "User weights:\n",
      " [-0.00405724 -0.04468237  0.01256463  0.0371066   0.00792453 -0.0283061\n",
      "  0.04394001 -0.03884629]\n",
      "Predicted user factors:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]]\n",
      "Reconstructed user factors:\n",
      "0.04394 -0.0388463\n",
      "0.0125646 0.0371066\n",
      "0.04394 -0.0388463\n",
      "0.00792453 -0.0283061\n"
     ]
    }
   ],
   "source": [
    "print('User index:\\n',X_train[0])\n",
    "print('User weights:\\n',np.concatenate(model.layers[2].get_weights()[0]))\n",
    "print('Predicted user factors:\\n',factrs[0])\n",
    "print('Reconstructed user factors:')\n",
    "for User in X_train[0]: print(np.concatenate(model.layers[2].get_weights()[0])[2*User],\n",
    "                             np.concatenate(model.layers[2].get_weights()[0])[2*User+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For item factors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User index:\n",
      " [3 1 3 2]\n",
      "User weights:\n",
      " [-0.00405724 -0.04468237  0.01256463  0.0371066   0.00792453 -0.0283061\n",
      "  0.04394001 -0.03884629]\n",
      "Predicted item factors:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]]\n",
      "Reconstructed item factors:\n",
      "0.0225021 0.0284794\n",
      "0.0225021 0.0284794\n",
      "-0.0432969 -0.0274621\n",
      "-0.0432969 -0.0274621\n"
     ]
    }
   ],
   "source": [
    "print('User index:\\n',X_train[0])\n",
    "print('User weights:\\n',np.concatenate(model.layers[2].get_weights()[0]))\n",
    "print('Predicted item factors:\\n',factrs[1])\n",
    "print('Reconstructed item factors:')\n",
    "for User in X_train[1]: print(np.concatenate(model.layers[3].get_weights()[0])[2*User],\n",
    "                             np.concatenate(model.layers[3].get_weights()[0])[2*User+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, factors for both users and items are the weights of the model indexed by the input values, but dimension of each factor is now `k_latent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding factors\n",
    "\n",
    "Addition of factors is necessary to calculate $S=\\sum_g f_g$ involved in calculation of interactions as part of $\\left(S-f_j\\right)$. \n",
    "\n",
    "In case of collaborative filtering the result becomes:\n",
    "$$ \\left[ v_j^{(u)}+v_h^{(i)} \\right]= \\left[ v_{j,1}^{(u)}+v_{h,1}^{(i)},\\ldots,v_{j,k}^{(u)}+v_{h,k}^{(i)} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_2/add:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Add()(factors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2)            0           input_12[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = [Input(shape=(2,)),Input(shape=(2,))]\n",
    "add = Model(inp,Add()(inp))\n",
    "plot_model(add, to_file='modelAdd.png',show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "add.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Add](modelAdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that output of this layer is equal to:\n",
    "\n",
    "- First column is first user factor, plus first item factor\n",
    "- Second column is second user factor, plus second item factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User factors:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]]\n",
      "\n",
      "Item factors:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]]\n",
      "\n",
      "Added columns of factors for user and for item:\n",
      " [[ 0.06644215 -0.01036687]\n",
      " [ 0.03506676  0.06558602]\n",
      " [ 0.00064315 -0.06630844]\n",
      " [-0.03537233 -0.05576825]]\n",
      "\n",
      "Result of Add layer:\n",
      " [[ 0.06644215 -0.01036687]\n",
      " [ 0.03506676  0.06558602]\n",
      " [ 0.00064315 -0.06630844]\n",
      " [-0.03537233 -0.05576825]]\n"
     ]
    }
   ],
   "source": [
    "s_pred=add.predict(factrs)\n",
    "print('User factors:\\n',factrs[0])\n",
    "print('\\nItem factors:\\n',factrs[1])\n",
    "print('\\nAdded columns of factors for user and for item:\\n',np.sum(factrs,axis=0))\n",
    "print('\\nResult of Add layer:\\n',s_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculation of term $\\left(S-f_j\\right)$\n",
    "\n",
    "To calculate difference $\\left(S-f_j\\right)$ use `Subtract` layer with precalculated tensors for $S$ and $f_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'subtract_2/sub:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'subtract_3/sub:0' shape=(?, ?) dtype=float32>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = [Subtract()([s, x]) for x in factors]\n",
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 2)            0           input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 2)            0           input_14[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s = Input(shape=(2,))\n",
    "fact = [Input(shape=(2,)),Input(shape=(2,))]\n",
    "diffs = Model([s]+fact,[Subtract()([s, x]) for x in fact])\n",
    "plot_model(diffs, to_file='modelDiff.png',show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "diffs.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Diff](modelDiff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The middle input tensor is $S$ calculated above by the Add layer, the other two inputs are user factors and item factors. The output is a list of 2 tensors: one for user and one for factor.\n",
    "\n",
    "Output for user is calculated as difference between matrix of sums `s_pred` and the user factor matrix. Correspondingly, difference between `s_pred` and item factor matrix is the output for item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User factors:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]] \n",
      "Item factors:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]]\n",
      "Sum of factors:\n",
      " [[ 0.06644215 -0.01036687]\n",
      " [ 0.03506676  0.06558602]\n",
      " [ 0.00064315 -0.06630844]\n",
      " [-0.03537233 -0.05576825]]\n",
      "Output of layer Subtract for user:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]] \n",
      "Output of layer Subtract for item:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]]\n",
      "Reconstructed layer Subtract for user:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]] \n",
      "Reconstructed layer Subtract for item:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]]\n"
     ]
    }
   ],
   "source": [
    "print('User factors:\\n',factrs[0],'\\nItem factors:\\n',factrs[1])\n",
    "print('Sum of factors:\\n',s_pred)\n",
    "dffs=diffs.predict([s_pred,factrs[0],factrs[1]])\n",
    "print('Output of layer Subtract for user:\\n',dffs[0],\n",
    "     '\\nOutput of layer Subtract for item:\\n',dffs[1])\n",
    "print('Reconstructed layer Subtract for user:\\n',np.subtract(s_pred,factrs[0]),\n",
    "     '\\nReconstructed layer Subtract for item:\\n',np.subtract(s_pred,factrs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Dot products\n",
    "\n",
    "Finally, everything is ready for calculation of the key expression of the model:\n",
    "$$\\sum_{j} (f_j \\cdot (S - f_j)),$$\n",
    "which is dot product of factors and the output of layer `Subtract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           input_17[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           input_18[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dif = [Input(shape=(2,)),Input(shape=(2,))]\n",
    "fact = [Input(shape=(2,)),Input(shape=(2,))]\n",
    "dots = Model(dif+fact,\n",
    "            [Dot(axes=1)([d, x]) for d,x in zip(dif, fact)])\n",
    "plot_model(dots, to_file='modelDot.png',show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "dots.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Dot](modelDot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left two input tensors are differences $(S - f_j)$ and factors for user and the right two input tensors are differences and factors for items.\n",
    "\n",
    "The output of the layer is list of dot product for user and for item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffs for user:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]] \n",
      "Diffs for item:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]]\n",
      "User factors:\n",
      " [[ 0.04394001 -0.03884629]\n",
      " [ 0.01256463  0.0371066 ]\n",
      " [ 0.04394001 -0.03884629]\n",
      " [ 0.00792453 -0.0283061 ]] \n",
      "Item factors:\n",
      " [[ 0.02250214  0.02847942]\n",
      " [ 0.02250214  0.02847942]\n",
      " [-0.04329686 -0.02746215]\n",
      " [-0.04329686 -0.02746215]]\n",
      "Output for users:\n",
      " [[-0.00011758]\n",
      " [ 0.00133951]\n",
      " [-0.00083566]\n",
      " [ 0.00043424]] \n",
      "Output for items:\n",
      " [[-0.00011758]\n",
      " [ 0.00133951]\n",
      " [-0.00083566]\n",
      " [ 0.00043424]]\n",
      "[-0.00011758  0.00133951 -0.00083566  0.00043424]\n",
      "[-0.00011758  0.00133951 -0.00083566  0.00043424]\n"
     ]
    }
   ],
   "source": [
    "dots_res = dots.predict([dffs[0],dffs[1],factrs[0],factrs[1]])\n",
    "print('Diffs for user:\\n',dffs[0],'\\nDiffs for item:\\n',dffs[1])\n",
    "print('User factors:\\n',factrs[0],'\\nItem factors:\\n',factrs[1])\n",
    "print('Output for users:\\n',dots_res[0],'\\nOutput for items:\\n',dots_res[1])\n",
    "print(np.diagonal(np.dot(dffs[0],factrs[0].transpose())))\n",
    "print(np.diagonal(np.dot(dffs[1],factrs[1].transpose())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps of the implementation are:\n",
    "\n",
    "- Concatenating linear terms and dots in order to put all terms of the model together:\n",
    "$$\\hat{y}_{j,h} = w_0+\\sum_{j=1}^{n_u} w_j^{(u)}x_j^{(u)} +\\sum_{h=1}^{n_i}w_h^{(i)}x_h^{(i)} + \\sum_{j}\\sum_{h} \\left(\\sum_{g=1}^k v_{j,g}^{(u)}v_{h,g}^{(i)}\\right) x_j^{(u)}x_h^{(i)},$$\n",
    "where the first 2 sums are linear terms and the all vectors of the last expression are returned by the `Dot` layer.\n",
    "- Adding batch normalization for stability\n",
    "- Adding the output layer with 1 unit and \"ReLU\" activation for positive rating prediction\n",
    "\n",
    "Note that in the final layer all terms will get their weights and will be added together to make the model formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "The following function puts all steps of LibFM algorithm togehter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(f_size):\n",
    "    dim_input = len(f_size)\n",
    "    \n",
    "    input_x = [Input(shape=(1,)) for i in range(dim_input)] \n",
    "    \n",
    "    lin_terms = [get_embed(x, size, 1) for (x, size) in zip(input_x, f_size)]\n",
    "\n",
    "    factors = [get_embed(x, size, k_latent) for (x, size) in zip(input_x, f_size)]\n",
    "     \n",
    "    s = Add()(factors)\n",
    "    \n",
    "    diffs = [Subtract()([s, x]) for x in factors]\n",
    "    \n",
    "    dots = [Dot(axes=1)([d, x]) for d,x in zip(diffs, factors)]\n",
    "    \n",
    "    x = Concatenate()(lin_terms + dots)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(1, activation='relu', kernel_regularizer=l2(kernel_reg))(x)\n",
    "    model = Model(inputs=input_x, outputs=[output])\n",
    "    model.compile(optimizer=Adam(clipnorm=0.25), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author uses parameter `clipnorm=0.5` in the Adam optimizer. This parameter is common for all Keras optimizers and enables avoiding so called \"Exploding Gradients\". Gradients will be clipped when their L2 norm exceeds clipnorm value.  \n",
    "\n",
    "It is recommended to tune this parameter to obtain better results.\n",
    "\n",
    "Check the complete model summary and plot its diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 2)         8           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 2)         10          input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2)            0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 2)            0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 2)            0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 1)         4           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 1)         5           input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_6 (Subtract)           (None, 2)            0           add_4[0][0]                      \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 2)            0           add_4[0][0]                      \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 1)            0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 1)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           subtract_6[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           subtract_7[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4)            0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 dot_3[0][0]                      \n",
      "                                                                 dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            5           batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 48\n",
      "Trainable params: 40\n",
      "Non-trainable params: 8\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(f_size)\n",
    "plot_model(model, to_file='modelComplete.png',show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Final model](modelComplete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Movielens-100k data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[944, 1683]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['user', 'movie']\n",
    "f_size  = [int(data_movielens[f].max()) + 1 for f in features]\n",
    "f_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 12, 780, 693, ...,  56, 548, 731]), array([ 238,  485,  215, ...,   11,  323, 1503])]\n",
      "[array([749, 539, 503, ..., 121, 401, 577]), array([ 23, 357, 729, ..., 515, 211,  31])]\n"
     ]
    }
   ],
   "source": [
    "train,test,y_train,y_test = train_test_split(data_movielens, data_movielens.rating.values, \n",
    "                                    stratify=data_movielens.rating.values, test_size=0.3)\n",
    "X_train = [train[f].values for f in features]\n",
    "X_test = [test[f].values for f in features]\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the model to train data. We stop training as soon as validation loss does not improve during 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  128\n",
      "Epoch 00025: early stopping\n",
      "\n",
      "\n",
      "1m04s\n",
      "RMSE 0.956454732649\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "batch_size = 128\n",
    "print('Batch size: ',batch_size)\n",
    "model = build_model(f_size)\n",
    "earlystopper = EarlyStopping(patience=2, verbose=1)\n",
    "t = mlc.time.Timer()\n",
    "model.fit(X_train,  y_train, \n",
    "          epochs=n_epochs, batch_size=batch_size, verbose=0, shuffle=True, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[earlystopper],\n",
    "         )\n",
    "print('\\n')\n",
    "print(t.fsince(0))\n",
    "print('RMSE',model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size,verbose=0)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Rogozhnikov compared different implementations of Libfm in his post [Testing implementations of LibFM](http://arogozhnikov.github.io/2016/02/15/TestingLibFM.html) and got the following table for sklearn LogisticRegression, original Libfm and two python packages fastFM and pylibfm.\n",
    "\n",
    "\n",
    "|package   | time  |RMSE   |\n",
    "|---|---|---|\n",
    "|  logistic | 0.059469  | 0.942771  |\n",
    "| libFM  |  8.970990 |  0.913520 |\n",
    "| fastFM  |  4.840041 | 0.915184  |\n",
    "| pylibfm  |  13.157164 | 0.944870  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The fit of the model in this notebook is in line with the reported results. \n",
    "\n",
    "However, the model can be improved by further tuning the model hyperparameters and by averaging predictions after several runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
